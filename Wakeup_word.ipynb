{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Install requirements"
      ],
      "metadata": {
        "id": "yrZJFta_MrZd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y_xPcQ87QyiT"
      },
      "outputs": [],
      "source": [
        "!pip install pytorch-lightning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V8ZhQlvBBR8W"
      },
      "outputs": [],
      "source": [
        "!pip install torchaudio-augmentations"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import libs"
      ],
      "metadata": {
        "id": "ai1M0P_LMxfl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5UivOxlj2xAc"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import librosa\n",
        "import random\n",
        "from IPython.display import Audio\n",
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torchaudio.transforms as T\n",
        "import soundfile as sf\n",
        "import torchaudio\n",
        "from tqdm.notebook import trange\n",
        "from torchaudio_augmentations import *\n",
        "PATH = Path('/content/drive/MyDrive/wakeup_word')\n",
        "\n",
        "os.listdir(PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Set up paths"
      ],
      "metadata": {
        "id": "PROQeTxjM6DQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dToYc1d83BeZ"
      },
      "outputs": [],
      "source": [
        "NO_WAKEUP = PATH/'no_wakeup'\n",
        "WAKEUP = PATH/'wakeup'\n",
        "wakeup_file = PATH/'thanos_message.wav'\n",
        "no_wakeup_file = PATH/'thanos_message_no_wakeup.wav'\n",
        "\n",
        "SAVE_SR = 16000\n",
        "\n",
        "num_segments = 1024"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load wakeup word file"
      ],
      "metadata": {
        "id": "OaMvMRfNM973"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SI8ha_h3H5zT"
      },
      "outputs": [],
      "source": [
        "waveform, sample_rate = torchaudio.load(wakeup_file)\n",
        "\n",
        "Audio(data=waveform, rate=sample_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Randomly sample and cut no wakup word parts"
      ],
      "metadata": {
        "id": "g9lA-F_Z_GmX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a63nM1bQ3mF1"
      },
      "outputs": [],
      "source": [
        "wav, sr = torchaudio.load(no_wakeup_file)\n",
        "\n",
        "cut_size = sr*1 # cut one second\n",
        "start_times = [random.randint(0, wav.shape[1] - cut_size) for _ in range(num_segments)]\n",
        "\n",
        "resampler = T.Resample(sr, SAVE_SR, dtype=wav.dtype)\n",
        "\n",
        "for i, start in enumerate(start_times):\n",
        "    end = start + cut_size\n",
        "    segment = wav[:,start:end]\n",
        "    resampled_waveform = resampler(segment)\n",
        "\n",
        "    save_path = NO_WAKEUP/f\"segment_{i}.wav\"\n",
        "    # torchaudio.save(save_path, resampled_waveform, SAVE_SR)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define audio augmentations"
      ],
      "metadata": {
        "id": "co7i9LqNNKoz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OlbmlHvI8fGU"
      },
      "outputs": [],
      "source": [
        "_rand_transforms = [\n",
        "    RandomApply([PolarityInversion()], p=0.1),\n",
        "    RandomApply([Noise(min_snr=0.1, max_snr=0.15)], p=0.1),\n",
        "    RandomApply([Gain()], p=0.1),\n",
        "    RandomApply([Delay(sample_rate=SAVE_SR)], p=0.1),\n",
        "    RandomApply([PitchShift(\n",
        "        n_samples=SAVE_SR//4,\n",
        "        sample_rate=SAVE_SR\n",
        "    )], p=0.1),\n",
        "    RandomApply([Reverb(sample_rate=SAVE_SR)], p=0.1)\n",
        "]\n",
        "\n",
        "transformator = Compose(transforms=_rand_transforms)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sample wakeup word with random cuts"
      ],
      "metadata": {
        "id": "lY2TJrzSNWv0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GfEfmPgJn8Qp"
      },
      "outputs": [],
      "source": [
        "num_segments=1024"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ncG2i7Fu7f-M"
      },
      "outputs": [],
      "source": [
        "waveform, sample_rate = torchaudio.load(wakeup_file)\n",
        "# waveform = waveform[:,:sample_rate]\n",
        "\n",
        "resampler = T.Resample(sample_rate, SAVE_SR)\n",
        "waveform = resampler(waveform)\n",
        "\n",
        "cut_size = 1*SAVE_SR\n",
        "start_times = [random.randint(0, waveform.shape[1] - cut_size) for _ in range(num_segments)]\n",
        "\n",
        "\n",
        "for i in trange(num_segments):\n",
        "  _st = start_times[i]\n",
        "  _end = _st+cut_size\n",
        "\n",
        "  transformed_audio =  transformator(waveform[:,_st:_end])\n",
        "  assert SAVE_SR == len(transformed_audio[0])\n",
        "\n",
        "  save_path = WAKEUP/f\"segment_{i}.wav\"\n",
        "  # torchaudio.save(save_path, transformed_audio, SAVE_SR)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create Dataset"
      ],
      "metadata": {
        "id": "FEDWCX8v5DZH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LH1NkOt6SjDH"
      },
      "outputs": [],
      "source": [
        "import torchaudio.transforms as T\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "class WakeupWordDataset(Dataset):\n",
        "    def __init__(self, audio_files, labels):\n",
        "        self.audio_files = audio_files\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        audio, sample_rate = torchaudio.load(self.audio_files[index])\n",
        "        mfcc_transform = T.MelSpectrogram(\n",
        "            sample_rate=sample_rate,\n",
        "            n_mels=80,\n",
        "            )\n",
        "        mfcc = mfcc_transform(audio[:,:16000]).squeeze()\n",
        "        label = torch.Tensor([self.labels[index]])\n",
        "        return mfcc.unsqueeze(0).float(), label\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.audio_files)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MZ3mUNBKKbkx"
      },
      "outputs": [],
      "source": [
        "wakeup_names = os.listdir(WAKEUP)\n",
        "no_wakeup_names = os.listdir(NO_WAKEUP)\n",
        "\n",
        "random.shuffle(wakeup_names)\n",
        "random.shuffle(no_wakeup_names)\n",
        "\n",
        "wakeup_names = list(map(lambda x: WAKEUP/x, wakeup_names))\n",
        "no_wakeup_names = list(map(lambda x: NO_WAKEUP/x, no_wakeup_names))\n",
        "\n",
        "train_size = int(num_segments*0.67)\n",
        "test_size = num_segments-train_size\n",
        "\n",
        "train_names = wakeup_names[:train_size] + no_wakeup_names[:train_size]\n",
        "\n",
        "\n",
        "train_labels = [1]*train_size+[0]*train_size\n",
        "\n",
        "test_names = wakeup_names[-test_size:] + no_wakeup_names[-test_size:]\n",
        "\n",
        "test_labels = [1]*test_size+[0]*test_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZNKoj1MjSj19"
      },
      "outputs": [],
      "source": [
        "train_dataset = WakeupWordDataset(train_names, train_labels)\n",
        "test_dataset = WakeupWordDataset(test_names, test_labels)\n",
        "\n",
        "batch_size = 16\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualization"
      ],
      "metadata": {
        "id": "YF3IQFkv6W0P"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ICF0Fq_3w2Pw"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "def plot_confusion_matrix(true_labels, predicted_labels):\n",
        "    # Compute confusion matrix\n",
        "    cm = confusion_matrix(true_labels, predicted_labels)\n",
        "\n",
        "    # Plot confusion matrix\n",
        "    fig, ax = plt.subplots()\n",
        "    im = ax.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
        "    ax.set(xticks=np.arange(cm.shape[1]),\n",
        "           yticks=np.arange(cm.shape[0]),\n",
        "           xticklabels=['Negative', 'Positive'],\n",
        "           yticklabels=['Negative', 'Positive'],\n",
        "           title='Confusion matrix',\n",
        "           ylabel='True label',\n",
        "           xlabel='Predicted label')\n",
        "\n",
        "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
        "             rotation_mode=\"anchor\")\n",
        "\n",
        "    # Loop over data dimensions and create text annotations.\n",
        "    for i in range(cm.shape[0]):\n",
        "        for j in range(cm.shape[1]):\n",
        "            ax.text(j, i, format(cm[i, j], 'd'),\n",
        "                    ha=\"center\", va=\"center\",\n",
        "                    color=\"white\" if cm[i, j] > cm.max() / 2. else \"black\")\n",
        "\n",
        "    fig.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define Model"
      ],
      "metadata": {
        "id": "RexAcms76aoR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o3A0_-mAQsh8"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "import pytorch_lightning as pl\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class WakeupWordCNN(pl.LightningModule):\n",
        "    def __init__(self):\n",
        "        super(WakeupWordCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 8, kernel_size=7, padding=0)\n",
        "        self.bn1 = nn.BatchNorm2d(8)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2)\n",
        "        self.fc1 = nn.Linear(8 * 37 * 37, 2)\n",
        "\n",
        "        self.training_step_accs = []\n",
        "        self.val_step_accs = []\n",
        "\n",
        "        self.test_preds=[]\n",
        "        self.test_labels=[]\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.pool(x)\n",
        "\n",
        "        x = x.view(-1,8*37 * 37)\n",
        "        x = self.fc1(x)\n",
        "        x = torch.sigmoid(x)\n",
        "        return x\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        mfcc, labels = batch\n",
        "        outputs = self(mfcc)\n",
        "        loss = F.binary_cross_entropy(outputs[:,1], labels.squeeze())\n",
        "        self.log('train_loss', loss, on_step=True, on_epoch=True)\n",
        "\n",
        "        preds = (outputs[:,1] > 0.5).float()\n",
        "        acc = (preds == labels.squeeze()).float().tolist()\n",
        "        self.training_step_accs.extend(acc)\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def on_train_epoch_end(self):\n",
        "        self.training_step_accs = []\n",
        "\n",
        "    def on_train_epoch_end(self):\n",
        "        acc = sum(self.training_step_accs) / len(self.training_step_accs)\n",
        "        self.log('train_acc_epoch', acc, on_step=False, on_epoch=True)\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        mfcc, labels = batch\n",
        "        outputs = self(mfcc)\n",
        "        loss = F.binary_cross_entropy(outputs[:,1], labels.squeeze())\n",
        "\n",
        "        preds = (outputs[:,1] > 0.5).float()\n",
        "        acc = (preds == labels.squeeze()).float().tolist()\n",
        "        self.val_step_accs.extend(acc)\n",
        "\n",
        "        self.log('val_loss', loss, on_step=True, on_epoch=True)\n",
        "\n",
        "\n",
        "    def on_validation_epoch_start(self):\n",
        "        self.val_step_accs = []\n",
        "\n",
        "\n",
        "    def on_validation_epoch_end(self):\n",
        "        acc = sum(self.val_step_accs) / len(self.val_step_accs)\n",
        "        self.log('val_acc_epoch', acc, on_step=False, on_epoch=True)\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        mfcc, labels = batch\n",
        "        outputs = self(mfcc)\n",
        "        loss = F.binary_cross_entropy(outputs[:,1], labels.squeeze())\n",
        "\n",
        "        preds = torch.argmax(outputs, dim=1)\n",
        "        acc = torch.sum(preds == labels.squeeze()).item() / len(labels)\n",
        "\n",
        "        self.test_preds.extend(preds.tolist())\n",
        "        self.test_labels.extend(labels.int().squeeze().tolist())\n",
        "\n",
        "    def on_test_epoch_start(self):\n",
        "        self.test_preds=[]\n",
        "        self.test_labels=[]\n",
        "\n",
        "    def on_test_epoch_end(self):\n",
        "        plot_confusion_matrix(self.test_labels, self.test_preds)\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = torch.optim.AdamW(self.parameters(), lr=5e-4)\n",
        "        return optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train"
      ],
      "metadata": {
        "id": "HimZg5vc-mBZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h8J_cpKga3Tq"
      },
      "outputs": [],
      "source": [
        "# %load_ext tensorboard\n",
        "# %tensorboard --logdir lightning_logs/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qE3hjRW4RILL"
      },
      "outputs": [],
      "source": [
        "model = WakeupWordCNN()\n",
        "\n",
        "trainer = pl.Trainer(max_epochs=10)\n",
        "trainer.fit(\n",
        "    model,\n",
        "    train_loader,\n",
        "    test_loader,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test"
      ],
      "metadata": {
        "id": "i8loCDNS-n4_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uORhlxwnW6gb"
      },
      "outputs": [],
      "source": [
        "trainer.test(\n",
        "    model,\n",
        "    test_loader\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Save scripted model"
      ],
      "metadata": {
        "id": "IiLyIwBF-qBC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gu42vNppsCaR"
      },
      "outputs": [],
      "source": [
        "script = model.to_torchscript(file_path=PATH/\"model.pt\", method=\"script\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load model"
      ],
      "metadata": {
        "id": "CL5oWvAc-vks"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from pathlib import Path\n",
        "\n",
        "PATH = Path('/content/drive/MyDrive/wakeup_word')\n",
        "model = torch.jit.load(PATH/\"model.pt\")\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "g07TJos1jnNk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Listen radio and catch wakeup words"
      ],
      "metadata": {
        "id": "VLOA64zIqr8k"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0E9QHGIbSWHh"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from pathlib import Path\n",
        "import requests\n",
        "import io\n",
        "import time\n",
        "import torchaudio\n",
        "from IPython.display import Audio\n",
        "\n",
        "PATH = Path('/content/drive/MyDrive/wakeup_word')\n",
        "model = torch.jit.load(PATH/\"model.pt\")\n",
        "model.eval()\n",
        "\n",
        "model.cuda()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def process_audio(wav, sample_rate):\n",
        "  \"\"\"Cuts wav with sliding window and makes predictions.\"\"\"\n",
        "  _wav = wav.unfold(-1,sample_rate, 2000)\n",
        "\n",
        "  mfcc_transform = torchaudio.transforms.MelSpectrogram(\n",
        "        sample_rate=sample_rate,\n",
        "        n_mels=80,\n",
        "  )\n",
        "\n",
        "  for i in range(_wav.shape[1]):\n",
        "      _cur_wav = _wav[:,i,:]\n",
        "\n",
        "      mfcc = mfcc_transform(_cur_wav).float().unsqueeze(0)\n",
        "      mfcc=mfcc.cuda()\n",
        "      out = model(mfcc)\n",
        "      out = (out[:,1]>0.95).float()\n",
        "      if 1==out.item():\n",
        "          print(i)\n",
        "          display(Audio(data=_cur_wav, rate=sample_rate))\n",
        "\n",
        "      mfcc=mfcc.cpu()\n",
        "  torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "rvv4N-1RqyjR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "audio_url = 'https://***/rario_stream'\n",
        "\n",
        "chunk_duration = 10\n",
        "stream_sample_rate = 44100\n",
        "output_buffer = io.BytesIO()\n",
        "sr=16000\n",
        "work_sample_rate = 16000\n",
        "resampler = torchaudio.transforms.Resample(sr, work_sample_rate)"
      ],
      "metadata": {
        "id": "sSiDjiKwq8iJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Listen stream. Display wakeup words"
      ],
      "metadata": {
        "id": "ticM1e4F_3Zw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output_buffer = io.BytesIO()\n",
        "while True:\n",
        "    response = requests.get(audio_url, stream=True)\n",
        "    for chunk in response.iter_content(chunk_size=4096):\n",
        "        output_buffer.write(chunk)\n",
        "        if output_buffer.getbuffer().nbytes >= (8000 * chunk_duration):\n",
        "            output_buffer.seek(0)\n",
        "            audio_data, sr = torchaudio.load(output_buffer)\n",
        "\n",
        "            resampled_waveform = resampler(audio_data)\n",
        "            process_audio(audio_data, work_sample_rate)\n",
        "            output_buffer = io.BytesIO()\n",
        "\n",
        "    if response.status_code==404:\n",
        "        print('status 404. so sad.')\n",
        "        time.sleep(15)"
      ],
      "metadata": {
        "id": "QVONRKchrWOz"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}